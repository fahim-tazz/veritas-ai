{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e956107d",
   "metadata": {},
   "source": [
    "#### Read TSV and label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "275eab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "pd.options.display.max_seq_items = None\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "cf6457a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_ohe = None\n",
    "title_ohe = None\n",
    "mlb_subject = None\n",
    "tfidf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c319f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    cols = [\"id\", \n",
    "            \"label\", \n",
    "            \"statement\", \n",
    "            \"subject\", \n",
    "            \"speaker\", \n",
    "            \"speaker_title\", \n",
    "            \"state\", \n",
    "            \"party\", \n",
    "            \"true_count\",\n",
    "            \"false_count\",\n",
    "            \"half_true_count\",\n",
    "            \"mostly_true_count\",\n",
    "            \"pof_count\",\n",
    "            \"context\"\n",
    "    ]\n",
    "    cols_map = {}\n",
    "    for i in range(len(cols)):\n",
    "        cols_map[i] = cols[i]\n",
    "\n",
    "    df = pd.read_table(path, header=None).rename(columns=cols_map)\n",
    "    # df.drop([\"id\", \"context\"], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c60db862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_XY(df):\n",
    "    return df.drop([\"binary_label\"], axis=1, inplace=False), df[\"binary_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "276992c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_zeros(df: DataFrame):\n",
    "    cols_to_impute = [\"true_count\", \"false_count\", \"half_true_count\", \"mostly_true_count\", \"pof_count\"]\n",
    "    df[cols_to_impute] = \\\n",
    "        df[cols_to_impute].fillna(0, inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc9014",
   "metadata": {},
   "source": [
    "#### One-hot Encode for *party*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "32db4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_party(df):\n",
    "    # Fill missing\n",
    "    df[\"party\"] = df[\"party\"].fillna(\"unknown\")\n",
    "\n",
    "    # One-hot encode\n",
    "    global party_ohe\n",
    "    if party_ohe is None:\n",
    "        party_ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "        party_encoded = party_ohe.fit_transform(df[[\"party\"]])\n",
    "    else:\n",
    "        party_encoded = party_ohe.transform(df[[\"party\"]])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    party_df = pd.DataFrame(party_encoded, columns=party_ohe.get_feature_names_out([\"party\"]))\n",
    "\n",
    "    # Concatenate with original dataframe\n",
    "    df = pd.concat([df.reset_index(drop=True), party_df.reset_index(drop=True)], axis=1)\n",
    "    df.drop([\"party\"], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733e9b",
   "metadata": {},
   "source": [
    "#### Multi-hot encoding for *subject* column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0a79b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_subject(df):\n",
    "    df[\"subject\"] = df[\"subject\"].fillna(\"\")\n",
    "    df[\"subject_split\"] = df[\"subject\"].str.split(\",\")\n",
    "\n",
    "    global mlb_subject\n",
    "    if mlb_subject is None:\n",
    "        mlb_subject = MultiLabelBinarizer()\n",
    "        subject_encoded = mlb_subject.fit_transform(df[\"subject_split\"])\n",
    "    else:\n",
    "        subject_encoded = mlb_subject.transform(df[\"subject_split\"])\n",
    "    subject_df = pd.DataFrame(subject_encoded, columns=mlb_subject.classes_)\n",
    "\n",
    "    subject_df = subject_df.reindex(columns=mlb_subject.classes_, fill_value=0)\n",
    "\n",
    "    df.drop([\"subject\", \"subject_split\"], axis=1, inplace=True)\n",
    "    df = pd.concat([df, subject_df], axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efa742",
   "metadata": {},
   "source": [
    "#### One hot encoding for Speaker Title (only top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ca92cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_speaker_title(df):\n",
    "    # Fill missing first\n",
    "    df[\"speaker_title\"] = df[\"speaker_title\"].fillna(\"unknown\")\n",
    "\n",
    "    # Get top 10 most frequent titles\n",
    "    top_titles = df[\"speaker_title\"].value_counts().nlargest(10).index\n",
    "\n",
    "    # Replace others with \"other\"\n",
    "    df[\"title\"] = df[\"speaker_title\"].where(df[\"speaker_title\"].isin(top_titles), \"other\")\n",
    "\n",
    "    # One-hot encode\n",
    "    global title_ohe\n",
    "    if title_ohe is None:\n",
    "        title_ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "        title_encoded = title_ohe.fit_transform(df[[\"title\"]])\n",
    "    else:\n",
    "        title_encoded = title_ohe.transform(df[[\"title\"]])\n",
    "\n",
    "    # Use consistent input column name!\n",
    "    title_df = pd.DataFrame(title_encoded, columns=title_ohe.get_feature_names_out([\"title\"]))\n",
    "\n",
    "    # Concatenate and clean up\n",
    "    df = pd.concat([df.reset_index(drop=True), title_df.reset_index(drop=True)], axis=1)\n",
    "    df.drop([\"title\", \"speaker_title\"], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0bddf",
   "metadata": {},
   "source": [
    "#### Map labels to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d1c434ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_labels(df):\n",
    "    def map_label(label):\n",
    "        if label in ['pants-fire', 'false', 'barely-true']:\n",
    "            return 0\n",
    "        elif label in ['half-true', 'mostly-true', 'true']:\n",
    "            return 1\n",
    "\n",
    "    df['binary_label'] = df['label'].apply(map_label)\n",
    "    df.drop([\"label\"], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee18652",
   "metadata": {},
   "source": [
    "#### Convert *Statement* to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "8878b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfidf(statement: pd.DataFrame) -> pd.DataFrame:\n",
    "    global tfidf\n",
    "    statement_series = statement[\"statement\"]\n",
    "    if tfidf is None:\n",
    "        tfidf = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "        tfidf_matrix = tfidf.fit_transform(statement_series)\n",
    "    else:\n",
    "        tfidf_matrix = tfidf.transform(statement_series)\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out()).reset_index(drop=True)\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e200f8b",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "12bc75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_pipeline(path):\n",
    "    df = load_data(path)\n",
    "    df = one_hot_party(df)\n",
    "    df = one_hot_speaker_title(df)\n",
    "    df = multi_hot_subject(df)\n",
    "    df.drop([\"id\", \"speaker\", \"state\", \"context\"], axis=1, inplace=True)\n",
    "    vectors = generate_tfidf(df[[\"statement\"]].copy())\n",
    "    df.drop([\"statement\"], axis=1, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, vectors], axis=1)\n",
    "    df = binarize_labels(df)\n",
    "    # print(\"Nulls before imputation\" + df.isnull().sum(), sep=\"\\n\")\n",
    "    df = impute_zeros(df)\n",
    "    X_train, y_train = split_XY(df)\n",
    "    return X_train, y_train\n",
    "\n",
    "def statements_only(path):\n",
    "    df = load_data(path)\n",
    "    df = binarize_labels(df)\n",
    "    \n",
    "    # Split X and y\n",
    "    y = df[\"binary_label\"]\n",
    "    # Vectorize statements (X only)\n",
    "    X = generate_tfidf(df[[\"statement\"]])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def history_only(path):\n",
    "    df = load_data(path)\n",
    "    df = binarize_labels(df)\n",
    "    df = impute_zeros(df)\n",
    "    y = df[\"binary_label\"]\n",
    "    X = df[[\"true_count\",\n",
    "            \"false_count\",\n",
    "            \"half_true_count\",\n",
    "            \"mostly_true_count\",\n",
    "            \"pof_count\"]]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03d8cd",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890797aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_valid, y_valid):\n",
    "    # model = LogisticRegression(max_iter=10000)\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l1\",          # or \"elasticnet\"\n",
    "        solver=\"saga\",         # required for l1 or elasticnet\n",
    "        # l1_ratio=0.5,          # only for elasticnet\n",
    "        # C=0.5,                 # play around with this\n",
    "        max_iter=10000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1              # if using CV later\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    print(\"Train Set Class Dist: \", print(y_train.value_counts(normalize=True)))\n",
    "    # Accuracy\n",
    "    print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
    "\n",
    "    # Detailed metrics like precision, recall, f1-score\n",
    "    print(classification_report(y_valid, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_valid, y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "beb1b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_label\n",
      "1    0.561719\n",
      "0    0.438281\n",
      "Name: proportion, dtype: float64\n",
      "Train Set Class Dist:  None\n",
      "Accuracy: 0.6440809968847352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.52      0.58       616\n",
      "           1       0.63      0.76      0.69       668\n",
      "\n",
      "    accuracy                           0.64      1284\n",
      "   macro avg       0.65      0.64      0.64      1284\n",
      "weighted avg       0.65      0.64      0.64      1284\n",
      "\n",
      "Confusion Matrix:\n",
      " [[319 297]\n",
      " [160 508]]\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = statements_only(\"train.tsv\")\n",
    "# X_valid,y_valid = statements_only(\"valid.tsv\")\n",
    "\n",
    "X_train, y_train = base_pipeline(\"train.tsv\")\n",
    "X_valid,y_valid = base_pipeline(\"valid.tsv\")\n",
    "\n",
    "# X_train, y_train = history_only(\"train.tsv\")\n",
    "# X_valid,y_valid = history_only(\"valid.tsv\")\n",
    "\n",
    "model = train_and_evaluate(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269083f",
   "metadata": {},
   "source": [
    "**Base Pipeline**\n",
    "```\n",
    "Accuracy: 0.6425233644859814\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.66      0.51      0.58       616\n",
    "           1       0.63      0.76      0.69       668\n",
    "\n",
    "    accuracy                           0.64      1284\n",
    "   macro avg       0.65      0.64      0.63      1284\n",
    "weighted avg       0.65      0.64      0.64      1284\n",
    "\n",
    "Confusion Matrix:\n",
    " [[317 299]\n",
    " [160 508]]\n",
    "```\n",
    "---\n",
    "\n",
    "**Statement only**\n",
    "```\n",
    "Accuracy: 0.5950155763239875\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.60      0.46      0.52       616\n",
    "           1       0.59      0.72      0.65       668\n",
    "\n",
    "    accuracy                           0.60      1284\n",
    "   macro avg       0.60      0.59      0.59      1284\n",
    "weighted avg       0.60      0.60      0.59      1284\n",
    "\n",
    "Confusion Matrix:\n",
    " [[285 331]\n",
    " [189 479]]\n",
    "```\n",
    "---\n",
    "**History only**\n",
    "```\n",
    "Accuracy: 0.5669781931464174\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.15      0.25       616\n",
    "           1       0.55      0.95      0.70       668\n",
    "\n",
    "    accuracy                           0.57      1284\n",
    "   macro avg       0.64      0.55      0.47      1284\n",
    "weighted avg       0.64      0.57      0.48      1284\n",
    "\n",
    "Confusion Matrix:\n",
    " [[ 94 522]\n",
    " [ 34 634]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b04696",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result = permutation_importance(model, X_valid, y_valid, n_repeats=10, random_state=42)\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()[-20:]\n",
    "\n",
    "plt.barh(X_valid.columns[sorted_idx], result.importances_mean[sorted_idx])\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2109s-ay2425s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
